{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "137e1b4c",
   "metadata": {},
   "source": [
    "# Analysing email network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d63372",
   "metadata": {},
   "source": [
    "## Running simulation for N times after 500 steps\n",
    "### NOTE: DO NOT RUN THE NOTEBOOK IN MAIN DIRECTORY ENSURE YOU MAKE A COPY AND POINT TO THE APPROPRIATE FILES\n",
    "The following code runs the simulation N_RUNS time and collects the resulting centrality and violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d4740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from model_new_1 import ProblemSolvingModel\n",
    "\n",
    "# Configuration\n",
    "N_RUNS = 50  # Number of times to run the model\n",
    "STEPS_PER_RUN = 500\n",
    "\n",
    "# Storage for aggregation\n",
    "# agent_sums[i] will store sum of violations for agent i across runs\n",
    "agent_sums = {} \n",
    "agent_centralities = {} # Store centrality (should be constant if graph is static)\n",
    "\n",
    "print(f\"Starting {N_RUNS} runs...\")\n",
    "\n",
    "for run_idx in range(N_RUNS):\n",
    "    # Initialize model with a new seed\n",
    "    seed = 42 + run_idx\n",
    "    model = ProblemSolvingModel(\n",
    "        K=50,\n",
    "        alpha=2,\n",
    "        obs_prob=0.01,\n",
    "        clause_interval=10,\n",
    "        R=2000,\n",
    "        setup_source=\"dataset\",\n",
    "        file_path=\"data/EmailManufacturing-copy.xml\",\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Run the model\n",
    "    for _ in range(STEPS_PER_RUN):\n",
    "        model.step()\n",
    "    \n",
    "    # Collect data from this run\n",
    "    # 1. Get graph and agents\n",
    "    G = model.network\n",
    "    agents = model.schedule.agents\n",
    "    agent_dict = {a.unique_id: a for a in agents}\n",
    "    \n",
    "    # 2. Calculate Centrality (re-calculating to be safe, though static)\n",
    "    deg_dict = dict(G.in_degree())\n",
    "    ######################################################\n",
    "    ###   unommment whiever centrality you need       #### \n",
    "    ######################################################\n",
    "#    deg_dict = nx.eigenvector_centrality(G)\n",
    "    max_deg = max(deg_dict.values()) if deg_dict else 1.0\n",
    "    max_deg = max(max_deg, 1.0)\n",
    "    \n",
    "    # 3. Aggregate\n",
    "    for pid in agent_dict:\n",
    "        # Get metrics\n",
    "        viol = agent_dict[pid].true_violations\n",
    "        centr = deg_dict.get(pid, 0) / max_deg\n",
    "        \n",
    "        # Store/Add violations\n",
    "        if pid not in agent_sums:\n",
    "            agent_sums[pid] = []\n",
    "            agent_centralities[pid] = centr # Store once\n",
    "        agent_sums[pid].append(viol)\n",
    "\n",
    "    print(f\"Run {run_idx+1}/{N_RUNS} complete.\")\n",
    "\n",
    "# --- Post-Processing ---\n",
    "\n",
    "ids = sorted(agent_sums.keys())\n",
    "x_vals = [] # Centrality\n",
    "y_vals = [] # Average Violations\n",
    "\n",
    "for pid in ids:\n",
    "    avg_viol = np.mean(agent_sums[pid])\n",
    "    x_vals.append(agent_centralities[pid])\n",
    "    y_vals.append(avg_viol)\n",
    "\n",
    "# --- Plotting ---\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_vals, y_vals, s=15, alpha=0.7, label='Agent Avg')\n",
    "\n",
    "# Add regression line\n",
    "\n",
    "# if len(x_vals) > 1:\n",
    "#     z = np.polyfit(x_vals, y_vals, 1)\n",
    "#     p = np.poly1d(z)\n",
    "#     plt.plot(x_vals, p(x_vals), \"k-\", linewidth=2, label=f'Trend (slope={z[0]:.2f})')\n",
    "\n",
    "plt.xlabel(\"Normalized In-Degree Centrality\")\n",
    "plt.ylabel(f\"Average Violations (over {N_RUNS} runs)\")\n",
    "plt.title(\"Agent Performance: Centrality vs. Avg Violations\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f981fdc",
   "metadata": {},
   "source": [
    "## Plot centrality v/s Violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b58d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_vals, y_vals, s=15, alpha=0.7, label='Agent Avg')\n",
    "#############################################################################\n",
    "# Add regression line , uncomment the following section for regression line #\n",
    "#############################################################################\n",
    "#if len(x_vals) > 1:\n",
    "#    z = np.polyfit(x_vals, y_vals, 1)\n",
    "#    p = np.poly1d(z)\n",
    "#    plt.plot(x_vals, p(x_vals), \"k-\", linewidth=2, label=f'Trend (slope={z[0]:.2f})')\n",
    "\n",
    "plt.xlabel(\"Eigenvector Centrality\")\n",
    "plt.ylabel(f\"Average Violations (over {N_RUNS} runs)\")\n",
    "plt.title(\"Agent Performance: Centrality vs. Avg Violations\")\n",
    "plt.legend()\n",
    "#plt.xscale(\"log\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.ylim(10,max(y_vals)+10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "#plt.savefig(\"output/violations-centr-email.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741e6a4",
   "metadata": {},
   "source": [
    "## Saving data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb987933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare the data structure\n",
    "# 'ids', 'agent_centralities', and 'agent_sums' come from the previous script\n",
    "ids = sorted(agent_sums.keys())\n",
    "export_rows = []\n",
    "\n",
    "for pid in ids:\n",
    "    violations_list = agent_sums[pid]\n",
    "    \n",
    "    row = {\n",
    "        \"Agent_ID\": pid,\n",
    "        \"Centrality\": agent_centralities[pid],\n",
    "        \"Average_Violations\": np.mean(violations_list),\n",
    "        \"Std_Dev_Violations\": np.std(violations_list),  # Helpful to check stability\n",
    "        \"Min_Violations\": np.min(violations_list),\n",
    "        \"Max_Violations\": np.max(violations_list),\n",
    "        \"Num_Runs\": N_RUNS\n",
    "    }\n",
    "    \n",
    "    # OPTIONAL: If you want every single run's value in the file\n",
    "    # for i, val in enumerate(violations_list):\n",
    "    #     row[f\"Run_{i+1}_Violations\"] = val\n",
    "        \n",
    "    export_rows.append(row)\n",
    "\n",
    "# 2. Create a DataFrame\n",
    "df = pd.DataFrame(export_rows)\n",
    "\n",
    "# 3. Save to CSV\n",
    "filename = \"agent_performance_summary.csv\"\n",
    "#filename = \"agent_performance_summary_eigenvector.csv\"\n",
    "\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"✅ Successfully saved data for {len(ids)} agents to '{filename}'\")\n",
    "print(df.head()) # Print first few rows to verify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b7fdc",
   "metadata": {},
   "source": [
    "## Matching nodes between graph and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493e4467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import datetime\n",
    "\n",
    "# 1. Load the CSV with real identities\n",
    "# Assuming CSV format: Sender;Recipient;EventDate\n",
    "csv_file = \"data/communication.csv\"  # Replace with your actual file name\n",
    "df = pd.read_csv(csv_file, sep=';')\n",
    "\n",
    "# Convert 'EventDate' to Unix timestamp to match the XML key\n",
    "# We use pd.to_datetime and then convert to int (seconds)\n",
    "df['Timestamp'] = pd.to_datetime(df['EventDate']).astype('int64') // 10**9\n",
    "\n",
    "\n",
    "print(\"CSV Data loaded:\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Load the GraphML/XML file\n",
    "graph_file = \"data/EmailManufacturing-copy.xml\" # Replace with your actual file name\n",
    "G = nx.read_graphml(graph_file)\n",
    "cnt=0\n",
    "for u, v, data in G.edges(data=True):\n",
    "    print(u,\"\\t\",v,\"\\t\",data[\"time\"])\n",
    "    if cnt>5: break\n",
    "    cnt+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a52a154",
   "metadata": {},
   "outputs": [],
   "source": [
    "times=[]\n",
    "js=[]\n",
    "xml_data={\n",
    "    \"is\":[],\n",
    "    \"js\":[],\n",
    "    \"time\":[]\n",
    "}\n",
    "print(len(G.edges()))\n",
    "for u, v, data in G.edges(data=True):\n",
    "    xml_data[\"is\"].append(u)\n",
    "    xml_data[\"js\"].append(v)\n",
    "    xml_data[\"time\"].append(data[\"time\"])\n",
    "    \n",
    "    times.append(data[\"time\"])\n",
    "df_graph=pd.DataFrame(xml_data)\n",
    "df_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f94be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph1=df_graph.sort_values(by=[\"time\",\"is\"],)\n",
    "\n",
    "\n",
    "# Aggregate Graph Data: Get unique senders per timestamp\n",
    "g_senders = df_graph1.groupby('time')['is'].unique().reset_index()\n",
    "g_senders.columns = ['time', 'graph_sender_list']\n",
    "g_senders\n",
    "# Aggregate CSV Data: Get unique senders per timestamp\n",
    "c_senders = df.groupby('Timestamp')['Sender'].unique().reset_index()\n",
    "c_senders.columns = ['time', 'csv_sender_list']\n",
    "\n",
    "# Merge on Time\n",
    "merged = pd.merge(g_senders, c_senders, on='time', how='inner')\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222ebe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Mapping Dictionary\n",
    "node_map = {}\n",
    "\n",
    "for idx, row in merged.iterrows():\n",
    "    g_list = row['graph_sender_list']\n",
    "    c_list = row['csv_sender_list']\n",
    "    \n",
    "    # Case 1: Perfect 1-to-1 match (Most common)\n",
    "    # Only one sender active at this second in both files\n",
    "    if len(g_list) == 1 and len(c_list) == 1:\n",
    "        node_map[g_list[0]] = c_list[0]\n",
    "        \n",
    "    # Case 2: Multiple senders active at the same second\n",
    "    # We can't distinguish them unless we look at the count of messages sent\n",
    "    elif len(g_list) > 1:\n",
    "        # Refine by checking message counts\n",
    "        # Get counts for this specific timestamp\n",
    "        g_counts = df_graph[df_graph['time'] == row['time']]['is'].value_counts()\n",
    "        c_counts = df[df['Timestamp'] == row['time']]['Sender'].value_counts()\n",
    "        \n",
    "        for g_node in g_list:\n",
    "            count = g_counts[g_node]\n",
    "            # Find csv node with same count\n",
    "            matches = c_counts[c_counts == count].index.tolist()\n",
    "            if len(matches) == 1:\n",
    "                node_map[g_node] = matches[0]\n",
    "\n",
    "# 3. Apply Mapping to DataFrame\n",
    "print(f\"Mapped {len(node_map)} unique sender nodes.\") #167 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fca305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary to DataFrame for saving\n",
    "mapping_df = pd.DataFrame(list(node_map.items()), columns=['Graph_Node', 'Real_ID'])\n",
    "print(mapping_df.head())\n",
    "\n",
    "# Save mapping\n",
    "mapping_df.to_csv(\"node_identity_mapping.csv\", index=False)\n",
    "\n",
    "# 4. Verify Mapping\n",
    "# Create a mapped version of the graph dataframe\n",
    "df_graph_mapped = df_graph.copy()\n",
    "df_graph_mapped['Real_Sender'] = df_graph_mapped['is'].map(node_map)\n",
    "\n",
    "print(\"\\nVerification (First 5 rows):\")\n",
    "print(df_graph_mapped.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33bea7",
   "metadata": {},
   "source": [
    "## Mapped 154 unqiue sender nodes but nodes that only recieve are left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aed38d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32634fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load Data & Previous Mapping\n",
    "# Assuming df_graph, df, and node_map (dictionary from previous step) exist\n",
    "# df_graph columns: ['is', 'js', 'time']\n",
    "# df columns: ['Sender', 'Recipient', 'Timestamp']\n",
    "\n",
    "# Apply known sender mappings to the graph dataframe\n",
    "df_graph['mapped_sender'] = df_graph['is'].map(node_map)\n",
    "\n",
    "# 2. Filter for Unmapped Nodes\n",
    "all_graph_nodes = set(df_graph['is']).union(set(df_graph['js']))\n",
    "mapped_nodes = set(node_map.keys())\n",
    "unmapped_nodes = all_graph_nodes - mapped_nodes\n",
    "\n",
    "print(f\"Total Nodes: {len(all_graph_nodes)}\")\n",
    "print(f\"Already Mapped: {len(mapped_nodes)}\")\n",
    "print(f\"Remaining Unmapped: {len(unmapped_nodes)}\")\n",
    "\n",
    "# 3. Create \"Receiver Fingerprints\"\n",
    "# A fingerprint is a set of (Sender_ID, Timestamp) tuples that a node received.\n",
    "\n",
    "# Fingerprint for Graph Nodes (using mapped sender IDs)\n",
    "graph_fingerprints = {}\n",
    "# Only look at rows where the target is unmapped\n",
    "candidates = df_graph[df_graph['js'].isin(unmapped_nodes)]\n",
    "\n",
    "for node, group in candidates.groupby('js'):\n",
    "    # Create a sorted tuple of (Sender, Time) events\n",
    "    # We rely on the 'mapped_sender' we already found\n",
    "    events = sorted(list(zip(group['mapped_sender'], group['time'])))\n",
    "    # Convert to tuple so it's hashable/comparable\n",
    "    graph_fingerprints[node] = tuple(events)\n",
    "\n",
    "# Fingerprint for CSV Nodes (Real Data)\n",
    "# We need to find which Real IDs are not yet in our values\n",
    "mapped_real_ids = set(node_map.values())\n",
    "all_real_ids = set(df['Sender']).union(set(df['Recipient']))\n",
    "unmapped_real_ids = all_real_ids - mapped_real_ids\n",
    "\n",
    "csv_fingerprints = {}\n",
    "candidates_csv = df[df['Recipient'].isin(unmapped_real_ids)]\n",
    "\n",
    "for node, group in candidates_csv.groupby('Recipient'):\n",
    "    events = sorted(list(zip(group['Sender'], group['Timestamp'])))\n",
    "    csv_fingerprints[node] = tuple(events)\n",
    "\n",
    "# 4. Match Fingerprints\n",
    "new_mappings = 0\n",
    "for g_node, g_fp in graph_fingerprints.items():\n",
    "    # Look for this fingerprint in the CSV fingerprints\n",
    "    # Inefficient loop but fine for 13 nodes\n",
    "    for c_node, c_fp in csv_fingerprints.items():\n",
    "        if g_fp == c_fp:\n",
    "            node_map[g_node] = c_node\n",
    "            new_mappings += 1\n",
    "            break\n",
    "\n",
    "print(f\"\\nSuccessfully mapped {new_mappings} pure receiver nodes.\")\n",
    "print(f\"Total Mapped: {len(node_map)}\")\n",
    "\n",
    "# 5. Save Final Full Mapping\n",
    "full_mapping_df = pd.DataFrame(list(node_map.items()), columns=['Graph_Node', 'Real_ID'])\n",
    "##################################################################\n",
    "##################################################################\n",
    "full_mapping_df.to_csv(\"full_node_mapping.csv\", index=False)\n",
    "##################################################################\n",
    "##################################################################\n",
    "print(\"Full mapping saved to 'full_node_mapping.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27121d9",
   "metadata": {},
   "source": [
    "## Finding node mapping of xml_ids to Agent_ID in ABM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9992f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_graph = nx.read_graphml(\"data/EmailManufacturing-copy.xml\")\n",
    "print(f\"Loaded graph from {filepath}\")\n",
    "print(f\"  Nodes: {loaded_graph.number_of_nodes()}\")\n",
    "print(f\"  Edges: {loaded_graph.number_of_edges()}\")\n",
    "print(f\"  Type: {'Directed' if loaded_graph.is_directed() else 'Undirected'}\")\n",
    "original_node_ids = list(loaded_graph.nodes())\n",
    "node_mapping = {orig_id: i for i, orig_id in enumerate(original_node_ids)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6805598a",
   "metadata": {},
   "source": [
    "### Validating mapping results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348bb4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Starting Validation ---\")\n",
    "\n",
    "# 1. Translate the Graph DataFrame\n",
    "df_translated = df_graph.copy()\n",
    "df_translated['Sender'] = df_translated['is'].map(node_map)\n",
    "df_translated['Recipient'] = df_translated['js'].map(node_map)\n",
    "df_translated['Timestamp'] = df_translated['time']\n",
    "\n",
    "# Drop unmapped rows if any (should be 0 if fully mapped)\n",
    "unmapped_edges = df_translated[df_translated['Sender'].isna() | df_translated['Recipient'].isna()]\n",
    "if len(unmapped_edges) > 0:\n",
    "    print(f\"⚠️ WARNING: {len(unmapped_edges)} edges could not be translated (nodes missing from map).\")\n",
    "    df_translated.dropna(subset=['Sender', 'Recipient'], inplace=True)\n",
    "\n",
    "# Ensure types match for comparison (int vs int)\n",
    "df_translated['Sender'] = df_translated['Sender'].astype(int)\n",
    "df_translated['Recipient'] = df_translated['Recipient'].astype(int)\n",
    "\n",
    "# --- Check 1: Set Comparison (Exact Match of Events) ---\n",
    "# Create a set of tuples (Sender, Recipient, Time) for both\n",
    "graph_events = set(zip(df_translated['Sender'], df_translated['Recipient'], df_translated['Timestamp']))\n",
    "csv_events = set(zip(df['Sender'], df['Recipient'], df['Timestamp']))\n",
    "\n",
    "common = graph_events.intersection(csv_events)\n",
    "missing_in_csv = graph_events - csv_events\n",
    "extra_in_csv = csv_events - graph_events\n",
    "\n",
    "print(f\"\\n1. Event Match Validation\")\n",
    "print(f\"   Total Graph Events: {len(graph_events)}\")\n",
    "print(f\"   Total CSV Events:   {len(csv_events)}\")\n",
    "print(f\"   ✅ Exact Matches:    {len(common)}\")\n",
    "print(f\"   ❌ Missing in CSV:   {len(missing_in_csv)}\")\n",
    "print(f\"   ❌ Extra in CSV:     {len(extra_in_csv)}\")\n",
    "\n",
    "match_rate = len(common) / len(graph_events) * 100\n",
    "print(f\"   -> Match Accuracy: {match_rate:.2f}%\")\n",
    "\n",
    "# --- Check 2: Node Activity Consistency ---\n",
    "# Compare number of messages sent/received per node\n",
    "def get_activity(df, role_col):\n",
    "    return df[role_col].value_counts().sort_index()\n",
    "\n",
    "# Senders\n",
    "g_counts = get_activity(df_translated, 'Sender')\n",
    "c_counts = get_activity(df, 'Sender')\n",
    "# Align indexes (some nodes might be missing in one)\n",
    "all_senders = sorted(set(g_counts.index) | set(c_counts.index))\n",
    "diff_s = pd.DataFrame({'Graph_Count': g_counts, 'CSV_Count': c_counts}).fillna(0)\n",
    "diff_s['Diff'] = diff_s['Graph_Count'] - diff_s['CSV_Count']\n",
    "\n",
    "print(f\"\\n2. Node Activity Consistency\")\n",
    "mismatched_senders = diff_s[diff_s['Diff'] != 0]\n",
    "if len(mismatched_senders) == 0:\n",
    "    print(\"   ✅ PERFECT: All nodes send the exact same number of messages.\")\n",
    "else:\n",
    "    print(f\"   ⚠️ MISMATCH: {len(mismatched_senders)} nodes have different sent counts.\")\n",
    "    print(mismatched_senders.head())\n",
    "\n",
    "# --- Check 3: Spot Check Specific Discrepancies ---\n",
    "if len(missing_in_csv) > 0:\n",
    "    print(\"\\n3. Sample Discrepancy (Graph event not found in CSV):\")\n",
    "    sample = list(missing_in_csv)[0]\n",
    "    print(f\"   Graph says: {sample}\")\n",
    "    # Check if it exists with slight time diff?\n",
    "    near_matches = df[\n",
    "        (csv['Sender'] == sample[0]) & \n",
    "        (csv['Recipient'] == sample[1])\n",
    "    ]\n",
    "    if not near_matches.empty:\n",
    "        print(\"   Closest CSV match found:\")\n",
    "        print(near_matches)\n",
    "    else:\n",
    "        print(\"   No similar event found in CSV.\")\n",
    "\n",
    "print(\"\\n--- Validation Complete ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c734e4",
   "metadata": {},
   "source": [
    "# Finding hierarchies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e307b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "\n",
    "# --- 1. Load Hierarchy Data ---\n",
    "csv_data = \"\"\"ID;ReportsToID\n",
    "1;152\n",
    "2;132\n",
    "3;152\n",
    "4;technical email account - not used by employees\n",
    "5;36\n",
    "6;163\n",
    "7;86\n",
    "8;85\n",
    "9;85\n",
    "10;technical email account - not used by employees\n",
    "11;148\n",
    "12;104\n",
    "13;36\n",
    "14;163\n",
    "15;31\n",
    "16;39\n",
    "17;70\n",
    "18;163\n",
    "19;136\n",
    "20;27\n",
    "21;technical email account - not used by employees\n",
    "22;104\n",
    "23;technical email account - not used by employees\n",
    "24;technical email account - not used by employees\n",
    "25;104\n",
    "26;technical email account - not used by employees\n",
    "27;86\n",
    "28;39\n",
    "29;76\n",
    "30;77\n",
    "31;143\n",
    "32;141\n",
    "33;76\n",
    "34;121\n",
    "35;90\n",
    "36;86\n",
    "37;118\n",
    "38;141\n",
    "39;136\n",
    "40;70\n",
    "41;90\n",
    "42;70\n",
    "43;123\n",
    "44;141\n",
    "45;77\n",
    "46;technical email account - not used by employees\n",
    "47;27\n",
    "48;154\n",
    "49;39\n",
    "50;152\n",
    "51;former employee account\n",
    "52;118\n",
    "53;85\n",
    "54;141\n",
    "55;141\n",
    "56;45\n",
    "57;27\n",
    "58;141\n",
    "59;47\n",
    "60;156\n",
    "61;104\n",
    "62;141\n",
    "63;104\n",
    "64;154\n",
    "65;27\n",
    "66;70\n",
    "67;85\n",
    "68;70\n",
    "69;86\n",
    "70;86\n",
    "71;163\n",
    "72;27\n",
    "73;136\n",
    "74;90\n",
    "75;former employee account\n",
    "76;69\n",
    "77;76\n",
    "78;104\n",
    "79;148\n",
    "80;148\n",
    "81;39\n",
    "82;121\n",
    "83;36\n",
    "84;159\n",
    "85;86\n",
    "86;86\n",
    "87;former employee account\n",
    "88;154\n",
    "89;70\n",
    "90;69\n",
    "91;29\n",
    "92;33\n",
    "93;former employee account\n",
    "94;29\n",
    "95;39\n",
    "96;36\n",
    "97;104\n",
    "98;104\n",
    "99;136\n",
    "100;36\n",
    "101;121\n",
    "102;36\n",
    "103;85\n",
    "104;86\n",
    "105;104\n",
    "106;36\n",
    "107;159\n",
    "108;29\n",
    "109;154\n",
    "110;163\n",
    "111;former employee account\n",
    "112;136\n",
    "113;104\n",
    "114;104\n",
    "115;123\n",
    "116;39\n",
    "117;143\n",
    "118;104\n",
    "119;90\n",
    "120;85\n",
    "121;86\n",
    "122;124\n",
    "123;136\n",
    "124;143\n",
    "125;141\n",
    "126;47\n",
    "127;90\n",
    "128;85\n",
    "129;90\n",
    "130;70\n",
    "131;159\n",
    "132;152\n",
    "133;141\n",
    "134;124\n",
    "135;36\n",
    "136;69\n",
    "137;69\n",
    "138;148\n",
    "139;former employee account\n",
    "140;39\n",
    "141;136\n",
    "142;143\n",
    "143;69\n",
    "144;19\n",
    "145;76\n",
    "146;141\n",
    "147;33\n",
    "148;86\n",
    "149;19\n",
    "150;162\n",
    "151;104\n",
    "152;69\n",
    "153;69\n",
    "154;90\n",
    "155;36\n",
    "156;86\n",
    "157;39\n",
    "158;129\n",
    "159;136\n",
    "160;148\n",
    "161;148\n",
    "162;69\n",
    "163;86\n",
    "164;137\n",
    "165;29\n",
    "166;159\n",
    "167;141\n",
    "\"\"\"\n",
    "df_hier = pd.read_csv(StringIO(csv_data), sep=';')\n",
    "\n",
    "# Clean & Build Graph\n",
    "df_clean = df_hier[pd.to_numeric(df_hier['ReportsToID'], errors='coerce').notnull()].copy()\n",
    "df_clean['ReportsToID'] = df_clean['ReportsToID'].astype(int)\n",
    "\n",
    "H = nx.DiGraph()\n",
    "\n",
    "# Explicitly add all nodes\n",
    "all_nodes = set(df_clean['ID']).union(set(df_clean['ReportsToID']))\n",
    "H.add_nodes_from(all_nodes)\n",
    "\n",
    "# Add edges (Manager -> Employee)\n",
    "for _, row in df_clean.iterrows():\n",
    "    employee = row['ID']\n",
    "    manager = row['ReportsToID']\n",
    "    if employee != manager: \n",
    "        H.add_edge(manager, employee) \n",
    "\n",
    "# Calculate Levels (Distance from CEO 86)\n",
    "try:\n",
    "    levels = nx.single_source_shortest_path_length(H, 86)\n",
    "except nx.NodeNotFound:\n",
    "    print(\"❌ Error: CEO node (86) not found in hierarchy graph.\")\n",
    "    levels = {}\n",
    "\n",
    "# Create DataFrame for Hierarchy Levels\n",
    "level_df = pd.DataFrame(list(levels.items()), columns=['Real_ID', 'Hierarchy_Level'])\n",
    "level_df['Real_ID'] = level_df['Real_ID'].astype(int)\n",
    "\n",
    "print(f\"✅ Hierarchy Levels computed for {len(level_df)} employees.\")\n",
    "\n",
    "\n",
    "# --- 2. Load Simulation Results & Apply Mapping (Corrected) ---\n",
    "try:\n",
    "    df_sim = pd.read_csv(\"agent_performance_summary.csv\")\n",
    "    df_map = pd.read_csv(\"full_node_mapping.csv\")\n",
    "    \n",
    "    # 1. Check format of Mapping file's keys\n",
    "    map_key_example = df_map['Graph_Node'].iloc[0] # likely 'n0'\n",
    "    \n",
    "    # 2. Check format of Simulation file's keys\n",
    "    sim_key_example = df_sim['Agent_ID'].iloc[0]   # likely 0 (int)\n",
    "    \n",
    "    print(f\"Mapping Key Type: {type(map_key_example)} (Example: {map_key_example})\")\n",
    "    print(f\"Sim Key Type:     {type(sim_key_example)} (Example: {sim_key_example})\")\n",
    "    \n",
    "    # 3. Standardize to match the MAPPING file format\n",
    "    # If mapping is 'n0', 'n1' (string) and sim is 0, 1 (int)\n",
    "    if isinstance(map_key_example, str) and str(map_key_example).startswith('n'):\n",
    "        print(\"-> Converting Simulation IDs to 'nX' format...\")\n",
    "        # Convert int to string with 'n' prefix\n",
    "        df_sim['Graph_Node'] = 'n' + df_sim['Agent_ID'].astype(str)\n",
    "    else:\n",
    "        # Just ensure types match (string vs string or int vs int)\n",
    "        print(\"-> Converting Simulation IDs to match Mapping type...\")\n",
    "        df_sim['Graph_Node'] = df_sim['Agent_ID'].astype(df_map['Graph_Node'].dtype)\n",
    "\n",
    "    # 4. Verify types before merge\n",
    "    print(f\"Sim 'Graph_Node' dtype: {df_sim['Graph_Node'].dtype}\")\n",
    "    print(f\"Map 'Graph_Node' dtype: {df_map['Graph_Node'].dtype}\")\n",
    "\n",
    "    # 5. Perform Translation\n",
    "    df_sim_mapped = pd.merge(df_sim, df_map, on='Graph_Node', how='inner')\n",
    "    \n",
    "    print(f\"✅ Successfully mapped {len(df_sim_mapped)} agents.\")\n",
    "    print(df_sim_mapped[['Graph_Node', 'Real_ID', 'Average_Violations']].head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during mapping: {e}\")\n",
    "    df_sim_mapped = pd.DataFrame()\n",
    "\n",
    "# --- 3. Merge with Hierarchy Data ---\n",
    "if not df_sim_mapped.empty:\n",
    "    # Merge on Real_ID (e.g., 86, 17, 36...)\n",
    "    merged_df = pd.merge(df_sim_mapped, level_df, on='Real_ID', how='inner')\n",
    "    \n",
    "    if merged_df.empty:\n",
    "        print(\"⚠️ Merge result empty! IDs did not match between Mapping and Hierarchy.\")\n",
    "    else:\n",
    "        # --- 4. Analysis ---\n",
    "        print(\"\\n--- Performance by Hierarchy Level ---\")\n",
    "        level_stats = merged_df.groupby('Hierarchy_Level')['Average_Violations'].describe()[['count', 'mean', 'min', 'max']]\n",
    "        print(level_stats)\n",
    "\n",
    "        print(\"\\n--- Top 5 Best Performing Agents (Least Violations) ---\")\n",
    "        top_agents = merged_df.sort_values('Average_Violations').head(5)\n",
    "        print(top_agents[['Real_ID', 'Hierarchy_Level', 'Average_Violations']])\n",
    "\n",
    "        # --- 5. Visualization ---\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        merged_df.boxplot(column='Average_Violations', by='Hierarchy_Level', grid=False, patch_artist=True)\n",
    "        plt.title('Violations Distribution by Hierarchy Level')\n",
    "        plt.suptitle('') \n",
    "        plt.xlabel('Hierarchy Level (0 = CEO)')\n",
    "        plt.ylabel('Average Violations')\n",
    "        plt.show()\n",
    "\n",
    "        # Save final report\n",
    "        merged_df.to_csv(\"hierarchy_performance_analysis_mapped.csv\", index=False)\n",
    "        print(\"\\n✅ Analysis saved to 'hierarchy_performance_analysis_mapped.csv'\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping analysis due to missing data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd503d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "\n",
    "# --- 1. Load & Build Hierarchy Graph (Corrected for Node 86) ---\n",
    "# OR load from file:\n",
    "# df_hier = pd.read_csv(\"reports_to.csv\", sep=';')\n",
    "df_hier = pd.read_csv(StringIO(csv_data), sep=';')\n",
    "\n",
    "# Clean data\n",
    "df_clean = df_hier[pd.to_numeric(df_hier['ReportsToID'], errors='coerce').notnull()].copy()\n",
    "df_clean['ReportsToID'] = df_clean['ReportsToID'].astype(int)\n",
    "\n",
    "# Build Graph\n",
    "H = nx.DiGraph()\n",
    "# CRITICAL FIX: Add all nodes explicitly first to ensure the CEO (86) exists\n",
    "all_nodes = set(df_clean['ID']).union(set(df_clean['ReportsToID']))\n",
    "H.add_nodes_from(all_nodes)\n",
    "\n",
    "for _, row in df_clean.iterrows():\n",
    "    if row['ID'] != row['ReportsToID']:\n",
    "        H.add_edge(row['ReportsToID'], row['ID']) # Edge: Manager -> Employee\n",
    "\n",
    "# Compute Levels (Distance from CEO #86)\n",
    "try:\n",
    "    levels = nx.single_source_shortest_path_length(H, source=86)\n",
    "    level_df = pd.DataFrame(list(levels.items()), columns=['Agent_ID', 'Hierarchy_Level'])\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating levels: {e}\")\n",
    "    level_df = pd.DataFrame()\n",
    "\n",
    "# --- 2. Load Simulation Centrality Data ---\n",
    "try:\n",
    "    # Load the summary file from the previous step\n",
    "    df_sim = pd.read_csv(\"agent_performance_summary.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ 'agent_performance_summary.csv' not found. Generating dummy data for demo.\")\n",
    "    # Dummy data generator for testing\n",
    "    import numpy as np\n",
    "    ids = list(levels.keys())\n",
    "    df_sim = pd.DataFrame({\n",
    "        'Agent_ID': ids, \n",
    "        'Centrality': np.random.rand(len(ids)) # Random centrality\n",
    "    })\n",
    "\n",
    "# --- 3. Merge Data ---\n",
    "# Inner merge ensures we only analyze nodes present in both the Hierarchy and the Simulation\n",
    "merged_df = pd.merge(df_sim, level_df, on='Agent_ID', how='inner')\n",
    "\n",
    "if merged_df.empty:\n",
    "    print(\"⚠️ Warning: Merge resulted in empty data. Check if 'Agent_ID's in simulation match 'ID's in hierarchy.\")\n",
    "else:\n",
    "    # --- 4. Statistical Analysis ---\n",
    "    print(\"\\n--- Degree Centrality by Hierarchy Level ---\")\n",
    "    # We want to see the Mean and Max centrality per level\n",
    "    stats = merged_df.groupby('Hierarchy_Level')['Centrality'].describe()[['count', 'mean', 'std', 'max']]\n",
    "    print(stats)\n",
    "\n",
    "    # Find the \"Informal Leaders\" (High centrality but low rank/high level number)\n",
    "    print(\"\\n--- Top 5 Most Central Agents ---\")\n",
    "    top_central = merged_df.sort_values('Centrality', ascending=False).head(5)\n",
    "    print(top_central[['Agent_ID', 'Hierarchy_Level', 'Centrality']])\n",
    "\n",
    "    # --- 5. Visualization ---\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create boxplot\n",
    "    merged_df.boxplot(column='Centrality', by='Hierarchy_Level', grid=False, patch_artist=True)\n",
    "    \n",
    "    plt.title('Communication Centrality vs. Corporate Hierarchy')\n",
    "    plt.suptitle('') # Removes default pandas subtitle\n",
    "    plt.xlabel('Hierarchy Level (0 = CEO, Higher = Lower Rank)')\n",
    "    plt.ylabel('Degree Centrality (Simulation)')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Save results\n",
    "    merged_df.to_csv(\"hierarchy_centrality_analysis.csv\", index=False)\n",
    "    print(\"\\n✅ Analysis saved to 'hierarchy_centrality_analysis.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03160f41",
   "metadata": {},
   "source": [
    "# Visualsing Company Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f752213",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "pos=nx.nx_agraph.graphviz_layout(H,prog=\"twopi\",args='-Granksep=0.25 -Gnodesep=3.0')\n",
    "nx.draw(H,with_labels=True,pos=pos,font_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c829506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "d = dict(nx.degree(H))\n",
    "# Ensure minimum node size so small nodes don't vanish\n",
    "node_sizes = [max(300, v * 200) for v in d.values()] \n",
    "pos = nx.nx_agraph.graphviz_layout(H, prog=\"dot\", args='-Grankdir=TB -Granksep=2.0 -Gnodesep=1.0')\n",
    "#pos=nx.nx_agraph.graphviz_layout(H,prog=\"dot\",args='-Granksep=0.25 -Gnodesep=3.0')\n",
    "nx.draw(H,with_labels=True,pos=pos,font_size=16,node_size=[v * 150 for v in d.values()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6718f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Assuming H is your graph\n",
    "plt.figure(figsize=(12, 12)) # Increased width for better separation\n",
    "\n",
    "# 1. Layout\n",
    "# Using 'dot' is perfect for hierarchies. increasing nodesep helps horizontal spacing.\n",
    "try:\n",
    "    pos = nx.nx_agraph.graphviz_layout(H, prog=\"twopi\", args='-Grankdir=TB -Granksep=2.0 -Gnodesep=1.0')\n",
    "except ImportError:\n",
    "    # Fallback if graphviz missing (e.g. on some systems)\n",
    "    pos = nx.kamada_kawai_layout(H) \n",
    "\n",
    "# 2. Metrics for styling\n",
    "d = dict(nx.degree(H))\n",
    "# Ensure minimum node size so small nodes don't vanish\n",
    "node_sizes = [max(300, v * 200) for v in d.values()] \n",
    "\n",
    "# 3. Draw Components Separately (Layered approach)\n",
    "\n",
    "# Layer 1: Edges (Light gray, curved looks better for hierarchies)\n",
    "nx.draw_networkx_edges(H, pos, edge_color='#AAAAAA', width=1.0, arrows=True, arrowsize=15, \n",
    "                       connectionstyle=\"arc3,rad=0.1\")\n",
    "\n",
    "# Layer 2: Nodes (Color by degree or hierarchy level)\n",
    "nodes = nx.draw_networkx_nodes(H, pos, node_size=node_sizes, node_color='#40a6d1', alpha=0.9)\n",
    "nodes.set_edgecolor('white') # Add border to nodes\n",
    "\n",
    "# Layer 3: Labels (The critical fix)\n",
    "# We create a custom label dictionary to filter out technical nodes if needed, or show all\n",
    "labels = {n: str(n) for n in H.nodes()}\n",
    "\n",
    "nx.draw_networkx_labels(H, pos, labels, font_size=10, font_weight='bold',\n",
    "                        bbox=dict(facecolor='white', edgecolor='none', alpha=0.7, boxstyle='round,pad=0.2'))\n",
    "\n",
    "plt.title(\"Company Hierarchy\", fontsize=20)\n",
    "plt.axis('off') # Clean background\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b65bc50",
   "metadata": {},
   "source": [
    "# Compiling Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim = pd.read_csv(\"agent_performance_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7c2c75",
   "metadata": {},
   "source": [
    "## Adding xml_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a460d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim = pd.read_csv(\"agent_performance_summary.csv\")\n",
    "df_sim = df_sim.assign(\n",
    "    xml_id=pd.Series([ \"n\"+str(id) for id in df_sim[\"Agent_ID\"]]).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f7edba",
   "metadata": {},
   "source": [
    "## Adding Real_id mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b22b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim = df_sim.assign(\n",
    "    Real_id=pd.Series(\n",
    "        [full_mapping_df[\"Real_ID\"][full_mapping_df[\"Graph_Node\"]==xml_id].values[0] for xml_id in df_sim[\"xml_id\"]]\n",
    "        ).values\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d9dffc",
   "metadata": {},
   "source": [
    "## Adding Hierarchy levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8436c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels=[]\n",
    "for agent_id in df_sim[\"Agent_ID\"]:\n",
    "    level=level_df[\"Hierarchy_Level\"][level_df[\"Agent_ID\"]==agent_id].values\n",
    "    if len(level)==0:\n",
    "        levels.append(-1)\n",
    "    else:\n",
    "        levels.append(int(level[0]))\n",
    "levels        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880e0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim = df_sim.assign(\n",
    "    level=pd.Series(\n",
    "        levels\n",
    "        ).values\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7567def0",
   "metadata": {},
   "source": [
    "# Plotting publication plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d55e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n--- Eigenvector Centrality by Hierarchy Level ---\")\n",
    "# We want to see the Mean and Max centrality per level\n",
    "stats = sorted_sim.groupby('level')['Centrality'].describe()[['count', 'mean', 'std', 'max']]\n",
    "print(stats)\n",
    "\n",
    "# Find the \"Informal Leaders\" (High centrality but low rank/high level number)\n",
    "print(\"\\n--- Top 5 Most Central Agents ---\")\n",
    "top_central = sorted_sim.sort_values('Centrality', ascending=False).head(5)\n",
    "print(top_central[['Agent_ID', 'level', 'Centrality']])\n",
    "\n",
    "# --- 5. Visualization ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create boxplot\n",
    "sorted_sim.boxplot(column='Centrality', by='level', grid=False, patch_artist=False)\n",
    "\n",
    "plt.title('Centrality vs. Corporate Hierarchy')\n",
    "plt.suptitle('') # Removes default pandas subtitle\n",
    "plt.xlabel('Hierarchy Level (0 = CEO)')\n",
    "plt.ylabel('Degree Centrality (Simulation)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Visualization ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create boxplot\n",
    "sorted_sim.boxplot(column='Average_Violations', by='level', grid=False, patch_artist=False)\n",
    "\n",
    "plt.title('Violations vs. Corporate Hierarchy')\n",
    "plt.suptitle('') # Removes default pandas subtitle\n",
    "plt.xlabel('Hierarchy Level (0 = CEO)')\n",
    "plt.ylabel('Average Violations (Simulation)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Update font size\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Define levels\n",
    "unique_levels = sorted(sorted_sim[\"level\"].unique())\n",
    "\n",
    "# Create a colormap instance\n",
    "cmap = plt.get_cmap(\"tab10\") \n",
    "\n",
    "# Define a list of markers to cycle through\n",
    "# 'o': circle, 's': square, '^': triangle_up, 'D': diamond, 'v': triangle_down, 'X': x, 'P': plus\n",
    "markers = ['o', 's', '^', 'D', 'v', 'X', 'P', '*', 'h', '<']\n",
    "\n",
    "for i, level in enumerate(unique_levels):\n",
    "    # Filter data for this level\n",
    "    subset = sorted_sim[sorted_sim[\"level\"] == level]\n",
    "    \n",
    "    # Label for legend\n",
    "    label_text = f\"Level {level}\"\n",
    "    if level == 0: \n",
    "        label_text = \"Level 0 (CEO)\"\n",
    "    if level == -1: \n",
    "        label_text = \"Level -1 (No Report)\"\n",
    "    \n",
    "    # Select marker for this level\n",
    "    # Special case for CEO if you want to force a specific one (e.g., Star '*')\n",
    "    if level == 0:\n",
    "        current_marker = '*' \n",
    "        current_size = 200 # Make CEO bigger\n",
    "    else:\n",
    "        current_marker = markers[i % len(markers)]\n",
    "        current_size = 60 # Default size\n",
    "    \n",
    "    # Scatter plot for this subset\n",
    "    plt.scatter(\n",
    "        subset.Centrality, \n",
    "        subset.Average_Violations, \n",
    "        s=current_size, \n",
    "        alpha=0.5, \n",
    "        label=label_text,\n",
    "        color=cmap(i % 10), \n",
    "        marker=current_marker\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Eigenvector Centrality\")\n",
    "plt.ylabel(f\"Average Violations (over {N_RUNS} runs)\")\n",
    "plt.title(\"Agent Performance: Centrality vs. Avg Violations\")\n",
    "\n",
    "# Place legend\n",
    "plt.legend(title=\"Hierarchy Level\", bbox_to_anchor=(1.05, 1), loc='best')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout() \n",
    "plt.savefig(\"output/violations-centr-email.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177058af",
   "metadata": {},
   "source": [
    "## Correlation between centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim_deg=pd.read_csv(\"Emails_Analysis.csv\")\n",
    "df_sim_eigen=pd.read_csv(\"Emails_Analysis_eigenvector.csv\")\n",
    "np.corrcoef(df_sim_deg[\"Centrality\"],df_sim_eigen[\"Centrality\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3353b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_sim_deg[\"Centrality\"],df_sim_eigen[\"Centrality\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
